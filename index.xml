<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kb&#39;s Blog</title>
    <link>https://kibromhft.github.io/</link>
    <description>Recent content on Kb&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Sep 2022 10:00:00 -0700</lastBuildDate><atom:link href="https://kibromhft.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jobs in the Digital Era</title>
      <link>https://kibromhft.github.io/posts/2023-01-03-automation/</link>
      <pubDate>Thu, 08 Sep 2022 10:00:00 -0700</pubDate>
      
      <guid>https://kibromhft.github.io/posts/2023-01-03-automation/</guid>
      <description>As we enter the digital age, one thing is certain: technology is not just a trend, it's a game-changer. With each passing day, technology is making its way into more and more industries, bringing about a variety of benefits for society such as increased efficiency, improved communication, and access to information. But, with change comes challenges, and one of the biggest concerns is the displacement of jobs.
Neural tangent kernel (NTK) (Jacot et al. 2018) is a kernel to explain the evolution of neural networks during training via gradient descent.</description>
    </item>
    
    <item>
      <title>Debugging Deep Learning Models</title>
      <link>https://kibromhft.github.io/posts/2022-06-09-vlm/</link>
      <pubDate>Thu, 09 Jun 2022 15:10:30 -0700</pubDate>
      
      <guid>https://kibromhft.github.io/posts/2022-06-09-vlm/</guid>
      <description>Processing images to generate text, such as image captioning and visual question-answering, has been studied for years. Traditionally such systems rely on an object detection network as a vision encoder to capture visual features and then produce text via a text decoder. Given a large amount of existing literature, in this post, I would like to only focus on one approach for solving vision language tasks, which is to extend pre-trained Debugging Deep Learning Models: Strategies and Best Practices to be capable of consuming visual signals.</description>
    </item>
    
    <item>
      <title>Causality in Machine Learning</title>
      <link>https://kibromhft.github.io/posts/2022-11-20-causality_in_ML/</link>
      <pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://kibromhft.github.io/posts/2022-11-20-causality_in_ML/</guid>
      <description>Are you tired of making predictions based on correlation rather than causation? Introducing Causality in Machine Learning, a cutting-edge approach to understanding the underlying causes of complex data patterns. By incorporating causal inference techniques, we can gain a deeper understanding of how different variables interact and affect each other, leading to more accurate predictions and informed decision-making.</description>
    </item>
    
    
    <item>
      <title>Demystifying Overfitting in Deep Neural Networks: Separating Fact from Fiction</title>
      <link>https://kibromhft.github.io/posts/2020-08-06-nas/</link>
      <pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://kibromhft.github.io/posts/2020-08-06-nas/</guid>
      <description>
	  Overfitting is often perceived as a major challenge in DNNs, leading to a lack of confidence in their ability to generalize to new data. As Neal Shusterman, the author of &quot;Unwind&quot;, once wrote: &quot;But remember that good intentions pave many roads. Not all of them lead to hell.&quot; However, the reality is that the severity of overfitting in DNNs is often overstated and can be effectively mitigated through various techniques.</description>
    </item>


    <item>
      <title>Disentangling the Latent Space: A Guide to Beta-VAE</title>
      <link>https://kibromhft.github.io/posts/2022-11-27-vae/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://kibromhft.github.io/posts/2022-11-27-vae/</guid>
      <description>
Autocoder is invented to reconstruct high-dimensional data using a neural network model with a narrow bottleneck layer in the middle (oops, this is probably not true for Variational Autoencoder, and we will investigate it in details in later sections). A nice byproduct is dimension reduction: the bottleneck layer captures a compressed latent encoding.</description>
    </item>

   
    
    <item>
      <title>FAQ</title>
      <link>https://kibromhft.github.io/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kibromhft.github.io/faq/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
