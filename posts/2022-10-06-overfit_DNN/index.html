<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Demystifying Overfitting in Deep Neural Networks: Separating Fact from Fiction | Kb&#39;s Blog</title>
<meta name="keywords" content="data, active-learning" />
<meta name="description" content="Demystifying Overfitting in Deep Neural Networks: Separating Fact from Fiction.">
<meta name="author" content="kibrom Haftu">
<link rel="canonical" href="https://kibromhft.github.io/posts/2022-10-06-overfit_DNN/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet_main.css"  rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://kibromhft.github.io/favicon_peach.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://kibromhft.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://kibromhft.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://kibromhft.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://kibromhft.github.io/safari-pinned-tab.svg">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.1/css/all.css" integrity="sha384-vp86vTRFVJgpjF9jiIGPEEqYqlDwgyBgEF109VFjmqGmIY/Y4HV4d3Gp2irVfcrp" crossorigin="anonymous">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-8161570-5', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="Causality in Machine Learning" />
<meta property="og:description" content="Overfitting is often perceived as a major challenge in DNNs, leading to a lack of confidence in their ability to generalize to new data. " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kibromhft.github.io/posts/2022-10-06-overfit_DNN/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-06T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-10-06T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Demystifying Overfitting in Deep Neural Networks: Separating Fact from Fiction"/>
<meta name="twitter:description" content="Overfitting is often perceived as a major challenge in DNNs, leading to a lack of confidence in their ability to generalize to new data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://kibromhft.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Causality in Machine Learning",
      "item": "https://kibromhft.github.io/posts/2022-10-06-overfit_DNN/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Causality in Machine Learning",
  "name": "Causality in Machine Learning",
  "description": "In ML, we often focus on the relationship between inputs and outputs. But, have you ever wondered if there is more to the story? Are there underlying relationships between variables that we're missing? It is here that causality comes into play. Causality refers to the relationship between an event (the cause) and a second event (the effect), where the second event is a result of the first. Understanding causality is crucial for making accurate predictions and informed decisions. By taking into account the causal relationships between variables, we can improve the performance of our models and gain deeper insights into the underlying mechanisms that generate the data.",
  "keywords": [
    "data", "active-learning"
  ],
  "articleBody": "In ML, we often focus on the relationship between inputs and outputs. But, have you ever wondered if there is more to the story? Are there underlying relationships between variables that we're missing? It is here that causality comes into play. Causality refers to the relationship between an event (the cause) and a second event (the effect), where the second event is a result of the first. Understanding causality is crucial for making accurate predictions and informed decisions. By taking into account the causal relationships between variables, we can improve the performance of our models and gain deeper insights into the underlying mechanisms that generate the data..\n",
  "wordCount" : "4584",
  "inLanguage": "en",
  "datePublished": "2022-10-06T00:00:00Z",
  "dateModified": "2022-10-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "kibrom Haftu"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kibromhft.github.io/posts/2022-10-06-overfit_DNN/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lil'Log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kibromhft.github.io/favicon_peach.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://kibromhft.github.io/" accesskey="h" title="Kb&#39;s Blog (Alt + H)">Kb&#39;s Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
       <ul id="menu">

		
		  <li>
			<a href="https://kibromhft.github.io/" title="Home">
			  Home <span class="caret fa fa-home"></span>
			</a>
		  </li>
		  

		  <li>
			<a href="https://kibromhft.github.io/archives/" title="Archives">
			  <span>Archives</span>
			</a>
		  </li>
		  


		  <li>
			<a href="https://kibromhft.github.io/data/" title="Data">
			  <span>Data</span>
			</a>
		  </li>
		  <li>
			<a href="https://kibromhft.github.io/keywords/" title="keywords">
			  <span>Keywords</span>
			</a>
		  </li>
		  <li>
			<a href="https://kibromhft.github.io/faq" title="FAQ">
			  <span>FAQs</span>
			</a>
		  </li>
		  
<!-- 		  <li>
			<a href="https://kibromhft.github.io/cv" title="cv">
			  <span>CV</span>
			</a>
		  </li> -->

		<li class="dropdown">
			<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
			  Insights <span class="caret fa fa-bars" data-toggle="dropdown"></span>
			</a>
		  <ul class="dropdown-menu" style="padding: 0px;">
			<li class="header-height first">
			  <a href="https://kibromhft.github.io/podcast">Podcast</a>
			</li>
			<li class="header-height separator" style="padding: 0px;>
			  <a href="#">African Innovators</a>
			</li>
			<li class="header-height separator" style="padding: 0px;>
			  <a href="#">Technology Trends</a>
			</li>
			<li class="header-height" style="padding: 0px;>
			  <a href="#">Resources</a>
			</li>
		  </ul>
		</li>
		
		</ul>
		
    </nav>
</header>


<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Demystifying Overfitting in Deep Neural Networks: Separating Fact from Fiction 
    </h1>
    <div class="post-meta"><span title='2022-10-06 00:00:00 +0000 UTC'>October 6, 2022</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;kibrom Haftu

</div>
  </header> 

  <div class="post-content">
  
<p>This blog post will explain what overfitting is, how it occurs in deep neural networks, and how to prevent it. We will also discuss some of the common misconceptions about overfitting and provide practical techniques for avoiding it.</p>
<p>Overfitting is often perceived as a major challenge in DNNs, leading to a lack of confidence in their ability to generalize to new data. As Neal Shusterman, the author of &ldquo;Unwind&rdquo;, once wrote: &ldquo;But remember that good intentions pave many roads. Not all of them lead to hell.&rdquo; However, the reality is that the severity of overfitting in DNNs is often overstated and can be effectively mitigated through various techniques.</p>
<p><strong>What is overfitting?</strong></p>
<p>Overfitting occurs when a model becomes too complex, resulting in it fitting noise in the training data rather than the underlying patterns. This leads to poor generalization performance on new data. This is like trying to fit a square peg into a round hole; no matter how hard you try, the peg will never fit as well as it would in the correct space. The same applies to an overfit model; no matter how hard it tries, it won't be able to generalize well. In DNNs, overfitting can arise from a variety of factors such as the number of layers, number of neurons, and activation functions used.</p>
<p>One way to mathematically understand overfitting is by considering the bias-variance tradeoff. The bias of a model refers to how well it can capture the underlying patterns in the data, while the variance refers to how sensitive the model is to changes in the training data. A high bias model may oversimplify the underlying patterns and perform poorly on both the training and test data, while a high variance model may fit the training data too closely and perform poorly on new data.</p>
<p><strong>Severity of overfitting in DNNs</strong></p>
<p>While it is true that DNNs are susceptible to overfitting, it is imperative to note that not all DNNs suffer from it to the same extent. The severity of overfitting depends on the complexity of the problem and the quality and quantity of the training data. In cases where there is limited training data or a high degree of noise in the data, DNNs may struggle to generalize, leading to overfitting. However, in cases where there is ample high-quality training data, DNNs can effectively capture the underlying patterns without overfitting. For example, a deep learning model trained on a high-resolution dataset of images with high-quality labels may be able to accurately classify objects in images without overfitting, while a model trained on a low-resolution dataset of images with noisy labels may struggle to accurately classify objects in the images.</p>
<p><strong>Techniques to Address Overfitting</strong></p>
<p>DNNs are equipped with various techniques to address overfitting. One such technique is regularization, which involves adding constraints to the model to reduce its complexity. Dropout, weight decay, and batch normalization are commonly used regularization techniques that can help prevent overfitting in DNNs. Dropout randomly drops neurons during training, reducing the model's dependence on specific neurons and improving generalization. Weight decay further penalizes overly large weights, further reducing the complexity of the model and helping to prevent overfitting. It adds a penalty term to the loss function to encourage smaller weights, which simplifies the model's complexity. Batch normalization helps to normalize the data across batches, resulting in a more consistent training process and further minimizing the risk of overfitting. It normalizes the inputs to each layer, reducing the dependence of the network on specific inputs.</p>
<p>Data augmentation is another powerful technique that can help prevent overfitting in DNNs. Data augmentation involves generating new training data by applying transformations such as rotation, scaling, and cropping. This increases the diversity of the training data, making the model more robust to variations in the input data. For instance, a simple data augmentation technique for images is to randomly flip the images horizontally and vertically, which can help the model generalize better and reduce overfitting.</p>
<p><strong>The Impact of Architecture Design</strong></p>
<p>Another factor that contributes to overfitting is the use of complex architectures in DNNs. Complex architectures such as deep neural networks with many layers or architectures with a large number of parameters increase the risk of overfitting. However, recent advancements in architecture design have led to the development of more efficient and effective models that are less prone to overfitting. For example, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are widely used in computer vision and natural language processing respectively, and are known for their ability to effectively learn from large datasets without overfitting.</p>
<p><strong>Transfer Learning</strong></p>
<p>Transfer learning is another technique that can be used to mitigate overfitting in DNNs. In transfer learning, a pre-trained model is used as a starting point for a new task, and the weights of the pre-trained model are fine-tuned on the new task. Transfer learning is like having an experienced mentor. They can provide you with helpful advice and support for tackling a new challenge, allowing you to skip some of the steps and mistakes that they have already gone through. It's like the difference between teaching someone to ride a bike by themselves or giving them a bike with training wheels first. The training wheels provide extra support and make it easier to learn how to ride the bike, but eventually, they must be removed in order to become a proficient rider. Similarily, it's much like the process of learning to swim: you can learn the basics with a floatation device, but you won't be able to swim independently until you remove the device and practice with the support of an experienced instructor. Transfer learning can be especially effective in cases where the new task has limited training data, as the pre-trained model has already learned useful features that can be leveraged for the new task. Transfer learning helps to reduce the amount of training data required to achieve good performance on a task, and can provide a good starting point for training a model. It also helps to improve generalization, as the pre-trained model has already been trained on a large dataset, which helps to reduce the risk of overfitting. For instance, a pre-trained model can be used to traina new AI application for medical diagnosis, which requires a much smaller dataset than would be needed to train the model from scratch.</p>
<p><strong>Hyperparameter Tuning</strong></p>
<p>Hyperparameters are parameters that are not learned during training, such as learning rate, batch size, and regularization strength. The values of these hyperparameters can significantly impact the performance of the model and its susceptibility to overfitting. Hyperparameter tuning involves searching for the optimal values of these hyperparameters that minimize the loss function on a validation set. Grid search, random search, and Bayesian optimization are commonly used techniques for hyperparameter tuning.</p>
<p><strong>Model Ensembling</strong></p>
<p>Model ensemble is a technique that involves combining multiple models to improve generalization performance. There are several ways to ensemble models, such as averaging the predictions of multiple models or using a weighted combination of models. Ensembling can help reduce the risk of overfitting by combining the strengths of multiple models and reducing the impact of individual model weaknesses.</p>
<p><strong>Conclusion</strong></p>
<p>Overfitting in DNNs is a significant challenge that can lead to poor generalization performance on new data. However, the severity of overfitting depends on the complexity of the problem and the quality and quantity of the training data. In cases where overfitting is a concern, there are various techniques available to address it, including regularization, data augmentation, careful architecture design, transfer learning, hyperparameter tuning, and model ensembling. Additionally, the choice of loss function plays a crucial role in the generalization ability of DNNs, and selecting an appropriate loss function can help improve generalization.</p>
<p>As the field of deep learning continues to evolve, it is likely that new techniques and approaches will emerge to address the challenge of overfitting in DNNs. Nonetheless, the current techniques discussed in this blog post provide a strong foundation for practitioners to effectively mitigate overfitting in their models and improve generalization performance on new data. By carefully considering the complexity of the problem, the quality and quantity of the training data, and the available techniques and approaches, practitioners can develop DNN models that generalize well to new data and provide valuable insights and predictions.</p>
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
	  <li><a href="https://kibromhft.github.io/keywords/Causality-in-AI/">overfitting</a></li>
      <li><a href="https://kibromhft.github.io/keywords/data/">data</a></li>
      <li><a href="https://kibromhft.github.io/keywords/machine-learning/">deep neural networks</a></li>
	  <li><a href="https://kibromhft.github.io/keywords/Transfer Learning/">Transfer Learning</a></li>
      <li><a href="https://kibromhft.github.io/keywords/regularization/">regularization</a></li>hyperparameters
	  
    </ul>



<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Causality in Machine Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=Causality%in%Machine%Learning&amp;url=https%3a%2f%2fkibromhft.github.io%2fposts%2f2022-10-06-overfit_DNN%2f&amp;hashtags=autoencoder%2cgenerative-model%2cimage-generation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Causality in Machine Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkibromhft.github.io%2fposts%2f2022-10-06-overfit_DNN%2f&amp;title=Causality%in%Machine%Learning&amp;summary=From%20Autoencoder%20to%20Beta-VAE&amp;source=https%3a%2f%2fkibromhft.github.io%2fposts%2f2022-10-06-overfit_DNN%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Causality in Machine Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fkibromhft.github.io%2fposts%2f2022-10-06-overfit_DNN%2f&title=Causality%in%Machine%Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
  

    <a target="_blank" rel="noopener noreferrer" aria-label="share Causality in Machine Learning on telegram"
        href="https://telegram.me/share/url?text=Causality%in%Machine%Learning&amp;url=https%3a%2f%2fkibromhft.github.io%2fposts%2f2022-10-06-overfit_DNN%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://kibromhft.github.io/">Kb&#39;s Blog</a></span>

</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g" style="visibility: visible; opacity: 1;">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <linearGradient id="grad" x1="0" y1="0" x2="0" y2="1">
            <stop offset="0%" stop-color="rgb(192, 132, 224)"/>
            <stop offset="100%" stop-color="rgb(153, 0, 204)"/>
        </linearGradient>
        <path d="M12 6H0l6-6z" fill="url(#grad)"></path>
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
